{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.12.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install datasets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py7zr\n",
      "  Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.6 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 30.7/67.6 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 67.6/67.6 kB 909.7 kB/s eta 0:00:00\n",
      "Collecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.1.0\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-win_amd64.whl (357 kB)\n",
      "     ---------------------------------------- 0.0/357.3 kB ? eta -:--:--\n",
      "     ------ -------------------------------- 61.4/357.3 kB 3.2 MB/s eta 0:00:01\n",
      "     ----------- -------------------------- 112.6/357.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 235.5/357.3 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 357.3/357.3 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting pybcj<1.1.0,>=1.0.0\n",
      "  Downloading pybcj-1.0.2-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Collecting pyppmd<1.2.0,>=1.1.0\n",
      "  Downloading pyppmd-1.1.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.1/46.1 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting psutil\n",
      "  Using cached psutil-5.9.8-cp37-abi3-win_amd64.whl (255 kB)\n",
      "Collecting pycryptodomex>=3.16.0\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/1.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.2/1.8 MB 3.0 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.8 MB 4.2 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.8 MB 4.2 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.8 MB 4.2 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.5/1.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.6/1.8 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.7/1.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.9/1.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.0/1.8 MB 2.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.2/1.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.4/1.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.6/1.8 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.7/1.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.8/1.8 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting texttable\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pyzstd>=0.15.9\n",
      "  Downloading pyzstd-0.15.10-cp310-cp310-win_amd64.whl (245 kB)\n",
      "     ---------------------------------------- 0.0/245.4 kB ? eta -:--:--\n",
      "     ----------------------------- ------- 194.6/245.4 kB 12.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 194.6/245.4 kB 12.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 194.6/245.4 kB 12.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 194.6/245.4 kB 12.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 194.6/245.4 kB 12.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 245.4/245.4 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting inflate64<1.1.0,>=1.0.0\n",
      "  Downloading inflate64-1.0.0-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, psutil, multivolumefile, inflate64, py7zr\n",
      "Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 psutil-5.9.8 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.10 texttable-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.12.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install py7zr\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "validation_file = 'validate.csv'\n",
    "\n",
    "dataset_config = {\n",
    "    'train': train_file,\n",
    "    'test': test_file,\n",
    "    'validation': validation_file\n",
    "}\n",
    "\n",
    "foren_dataset = load_dataset('csv', data_files=dataset_config)\n",
    "\n",
    "for split in foren_dataset.keys():\n",
    "    print(f\"{split} set - Num rows: {len(foren_dataset[split])}\")\n",
    "\n",
    "train_dataset = foren_dataset['train']\n",
    "test_dataset = foren_dataset['test']\n",
    "validation_dataset = foren_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [len(foren_dataset[split]) for split in foren_dataset]\n",
    "print(f\"{split_lengths}\")\n",
    "print(f\"{foren_dataset['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(foren_dataset[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(foren_dataset[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"kabita-choudhary/finetuned-bart-for-conversation-summary\")\n",
    "pipe_out = pipe(foren_dataset[\"test\"][0][\"dialogue\"],min_length=5, max_length=20)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"kabita-choudhary/finetuned-bart-for-conversation-summary\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_len = [len(tokenizer.encode(s)) for s in foren_dataset[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in foren_dataset[\"train\"][\"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.as_target_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=256, truncation=True)\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"], \n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],  \n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foren_dataset_p = foren_dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "foren_dataset_p.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "all_summaries = []\n",
    "all_targets = []\n",
    "\n",
    "for example in foren_dataset[\"test\"]:\n",
    "    inputs = tokenizer(example[\"dialogue\"], return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\").to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(**inputs)\n",
    "        summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    assert len(summaries) == 1, \"More than one summary generated for an example\"\n",
    "    \n",
    "    all_summaries.extend(summaries)\n",
    "    all_targets.append(example[\"summary\"])  \n",
    "\n",
    "print(\"Length of all_summaries:\", len(all_summaries))\n",
    "print(\"Length of all_targets:\", len(all_targets))\n",
    "\n",
    "assert len(all_summaries) == len(all_targets), \"Mismatch in the number of predictions and references\"\n",
    "\n",
    "\n",
    "rouge_result = rouge_metric.compute(predictions=all_summaries, references=all_targets)\n",
    "print(\"ROUGE Scores:\", rouge_result)\n",
    "\n",
    "bleu_score = corpus_bleu([[ref] for ref in all_targets], all_summaries)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments( output_dir=\"bart-large-cnn-samsum\",\n",
    "                                 num_train_epochs=3,\n",
    "                                 warmup_steps=500,\n",
    "                                 per_device_train_batch_size=1,\n",
    "                                 per_gpu_eval_batch_size=1,\n",
    "                                 weight_decay=0.01,\n",
    "                                 logging_steps=10,\n",
    "                                 evaluation_strategy='steps', \n",
    "                                 eval_steps=500,\n",
    "                                 save_steps=1e6,\n",
    "                                 gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                 args=training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 data_collator=seq2seq_data_collator,\n",
    "                 train_dataset=foren_dataset_p[\"train\"],\n",
    "                 eval_dataset=foren_dataset_p[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"foren_summer_bart_large\")\n",
    "tokenizer.save_pretrained(\"foren_summer_bart_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "model_path = \"/kaggle/working/foren_summer_bart_large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "all_summaries = []\n",
    "all_targets = []\n",
    "\n",
    "for example in foren_dataset[\"test\"]:\n",
    "    inputs = tokenizer(example[\"dialogue\"], return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(**inputs)\n",
    "        summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    assert len(summaries) == 1, \"More than one summary generated for an example\"\n",
    "    \n",
    "    all_summaries.extend(summaries)\n",
    "    all_targets.append(example[\"summary\"])  \n",
    "\n",
    "print(\"Length of all_summaries:\", len(all_summaries))\n",
    "print(\"Length of all_targets:\", len(all_targets))\n",
    "\n",
    "assert len(all_summaries) == len(all_targets), \"Mismatch in the number of predictions and references\"\n",
    "\n",
    "rouge_result = rouge_metric.compute(predictions=all_summaries, references=all_targets)\n",
    "print(\"ROUGE Scores:\", rouge_result)\n",
    "\n",
    "bleu_score = corpus_bleu([[ref] for ref in all_targets], all_summaries)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4556189,
     "sourceId": 7784885,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
